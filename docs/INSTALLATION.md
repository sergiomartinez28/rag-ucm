# Gu√≠a de Instalaci√≥n y Uso - RAG-UCM

## üìã Tabla de Contenidos

1. [Requisitos](#requisitos)
2. [Instalaci√≥n](#instalaci√≥n)
3. [Configuraci√≥n](#configuraci√≥n)
4. [Preparaci√≥n de Datos](#preparaci√≥n-de-datos)
5. [Uso del Sistema](#uso-del-sistema)
6. [Evaluaci√≥n](#evaluaci√≥n)
7. [Troubleshooting](#troubleshooting)

---

## üîß Requisitos

### Hardware m√≠nimo
- **RAM**: 8GB (16GB recomendado)
- **Disco**: 10GB libres
- **CPU**: 4 cores recomendados
- **GPU**: Opcional (acelera generaci√≥n, pero funciona en CPU)

### Software
- Python 3.10 o superior
- pip (gestor de paquetes Python)
- Git (opcional, para clonar el repositorio)

---

## üì¶ Instalaci√≥n

### Opci√≥n 1: Instalaci√≥n local

```bash
# 1. Navegar al directorio del proyecto
cd "rag-ucm"

# 2. Crear entorno virtual
python -m venv venv

# 3. Activar entorno virtual
# En macOS/Linux:
source venv/bin/activate
# En Windows:
venv\Scripts\activate

# 4. Actualizar pip
pip install --upgrade pip

# 5. Instalar dependencias
pip install -r requirements.txt

# 6. Copiar configuraci√≥n de ejemplo
cp .env.example .env
```

### Opci√≥n 2: Con Docker

```bash
# Construir imagen
docker build -t rag-ucm .

# Ejecutar contenedor
docker run -p 8501:8501 -v $(pwd)/data:/app/data rag-ucm
```

---

## ‚öôÔ∏è Configuraci√≥n

Edita el archivo `.env` con tus preferencias:

```bash
# Modelos (puedes cambiarlos seg√∫n disponibilidad)
EMBEDDING_MODEL=BAAI/bge-m3
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
LLM_MODEL=meta-llama/Llama-3.2-3B-Instruct

# Alternativas de LLM:
# LLM_MODEL=microsoft/Phi-3-mini-4k-instruct
# LLM_MODEL=Qwen/Qwen2.5-3B-Instruct

# Par√°metros de chunking
CHUNK_SIZE=600
CHUNK_OVERLAP=100

# Par√°metros de recuperaci√≥n
TOP_K_RETRIEVAL=20
TOP_K_RERANK=5
HYBRID_ALPHA=0.5  # 0=solo BM25, 1=solo embeddings

# Par√°metros de generaci√≥n
MAX_NEW_TOKENS=512
TEMPERATURE=0.3

# Verificaci√≥n
ENABLE_VERIFICATION=true
VERIFICATION_THRESHOLD=0.7
```

---

## üìö Preparaci√≥n de Datos

### 1. Obtener documentos

Descarga documentos oficiales de la UCM:

- Normativas TFG/TFM de tu facultad
- Calendarios acad√©micos
- Normativa de permanencia
- Gu√≠as de procedimientos

Col√≥calos en `data/raw/`

### 2. Construir √≠ndices

```bash
# Usando CLI
python cli.py build --path ./data/raw

# El proceso puede tardar varios minutos
# Ver√°s el progreso en la terminal
```

### 3. Verificar √≠ndices

```bash
# Ver estad√≠sticas
python cli.py stats
```

Deber√≠as ver algo como:

```
üìä Estad√≠sticas RAG-UCM

√çndices:
  ‚Ä¢ Total chunks: 245
  ‚Ä¢ Vectores FAISS: 245
  ‚Ä¢ Modelo embeddings: bge-m3
  ‚Ä¢ Dimensi√≥n: 1024
  ‚Ä¢ Longitud promedio: 487 palabras
```

---

## üöÄ Uso del Sistema

### Interfaz Web (Streamlit)

```bash
streamlit run app.py
```

Abre tu navegador en http://localhost:8501

**Caracter√≠sticas:**
- Interfaz visual amigable
- Configuraci√≥n en tiempo real
- Visualizaci√≥n de fuentes
- M√©tricas de verificaci√≥n

### L√≠nea de Comandos (CLI)

#### Hacer una pregunta

```bash
python cli.py ask "¬øCu√°l es el plazo para presentar el TFM?"
```

#### Con opciones avanzadas

```bash
python cli.py ask "¬øCu√°ntos cr√©ditos puedo convalidar?" \
  --top-k 7 \
  --verbose
```

#### Modo interactivo

```bash
python cli.py interactive
```

Permite hacer m√∫ltiples preguntas en una sesi√≥n.

### Como librer√≠a Python

```python
from src.pipeline import RAGPipeline

# Inicializar
rag = RAGPipeline()

# Hacer pregunta
result = rag.query("¬øCu√°ndo es el plazo del TFG?")

# Mostrar respuesta
print(result['answer'])

# Mostrar fuentes
for source in result['sources']:
    print(f"[{source['id']}] {source['title']}")

# Verificaci√≥n
if 'verification' in result:
    print(f"Fidelidad: {result['verification']['fidelity_score']:.2%}")
```

---

## üìä Evaluaci√≥n

### Crear conjunto de evaluaci√≥n

Crea un archivo `evaluation/questions.json`:

```json
[
  {
    "question": "¬øCu√°l es el plazo para presentar el TFM?",
    "expected_answer": "El plazo es...",
    "source_doc": "normativa_tfm_2024.pdf"
  },
  ...
]
```

### Ejecutar evaluaci√≥n (TODO: implementar)

```bash
python scripts/evaluate.py --questions evaluation/questions.json
```

### M√©tricas RAGAS

El sistema incluye verificaci√≥n autom√°tica con m√©tricas de:
- **Fidelidad**: ¬øLa respuesta est√° respaldada por los documentos?
- **Relevancia**: ¬øLos documentos recuperados son relevantes?
- **Completitud**: ¬øLa respuesta es completa?

---

## üîß Troubleshooting

### Problema: "No se encuentran √≠ndices"

**Soluci√≥n**: Ejecuta `python cli.py build` primero.

### Problema: "Out of memory"

**Soluciones**:
1. Reduce `CHUNK_SIZE` en `.env`
2. Reduce `TOP_K_RETRIEVAL`
3. Usa un LLM m√°s peque√±o (Phi-3-mini)
4. Cierra otras aplicaciones

### Problema: "Modelo no encontrado"

**Soluci√≥n**: Los modelos se descargan autom√°ticamente de HuggingFace la primera vez. Aseg√∫rate de tener conexi√≥n a internet.

### Problema: Respuestas lentas

**Soluciones**:
1. Si tienes GPU NVIDIA, instala `torch` con CUDA:
   ```bash
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   ```
2. Reduce `TOP_K_RERANK` a 3
3. Usa un modelo m√°s peque√±o

### Problema: El sistema "alucina" (inventa informaci√≥n)

**Soluciones**:
1. Activa `ENABLE_VERIFICATION=true`
2. Reduce `TEMPERATURE` a 0.1-0.2
3. Aumenta `VERIFICATION_THRESHOLD`
4. Revisa que los documentos sean completos y claros

---

## üìù Siguiente Pasos

1. **Expandir documentos**: A√±ade m√°s normativas a `data/raw/`
2. **Fine-tuning**: Considera hacer fine-tuning del LLM con ejemplos UCM
3. **Evaluaci√≥n formal**: Crea un conjunto de test con 100+ preguntas
4. **Despliegue**: Dockeriza y despliega en servidor interno

---

## üÜò Soporte

Para dudas o problemas:
1. Revisa la documentaci√≥n en `docs/`
2. Consulta el README principal
3. Contacta: sergma22@ucm.es

---

**¬°Buena suerte con tu TFM! üéì**
