{
  "precision_at_k": 0.8367346938775511,
  "precision_at_k_chunk": 0.726530612244898,
  "mrr": 0.7040136054421767,
  "avg_relevancia": 0.8453061224489811,
  "avg_fidelidad": 0.7816326530612262,
  "avg_precision": 0.6314285714285721,
  "avg_overall": 0.752789115646259,
  "avg_retrieval_time": 0.12309849700149225,
  "avg_generation_time": 7.0005049228668215,
  "avg_total_time": 7.124264453381908,
  "total_questions": 245,
  "questions_with_correct_doc": 205,
  "questions_with_correct_chunk": 178,
  "abstention_rate": 0.0,
  "suspicious_short_rate": 0.0,
  "factual_accuracy": 0.5510204081632653,
  "abstention_when_reference_exists": 0.0,
  "abstention_when_chunk_correct": 0.0,
  "abstention_when_doc_correct": 0.0,
  "abstention_with_numbers_in_ctx": 0.0,
  "abstention_with_keywords_in_ctx": 0.0,
  "metrics_by_category": {
    "estatutos": {
      "count": 39,
      "precision_at_k": 0.8205128205128205,
      "avg_relevancia": 0.8076923076923073,
      "avg_fidelidad": 0.7461538461538462,
      "avg_precision": 0.605128205128205,
      "avg_overall": 0.7196581196581195,
      "avg_time": 7.089812449919871
    },
    "general": {
      "count": 74,
      "precision_at_k": 0.9054054054054054,
      "avg_relevancia": 0.863513513513513,
      "avg_fidelidad": 0.7905405405405401,
      "avg_precision": 0.6851351351351349,
      "avg_overall": 0.7797297297297294,
      "avg_time": 6.793866708471969
    },
    "gobierno": {
      "count": 25,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8639999999999998,
      "avg_fidelidad": 0.812,
      "avg_precision": 0.708,
      "avg_overall": 0.7946666666666666,
      "avg_time": 6.770980043411255
    },
    "reglamentos": {
      "count": 89,
      "precision_at_k": 0.7191011235955056,
      "avg_relevancia": 0.8415730337078654,
      "avg_fidelidad": 0.7707865168539321,
      "avg_precision": 0.5797752808988764,
      "avg_overall": 0.7307116104868911,
      "avg_time": 7.228874262799038
    },
    "becas": {
      "count": 5,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8600000000000001,
      "avg_fidelidad": 0.8800000000000001,
      "avg_precision": 0.68,
      "avg_overall": 0.8066666666666666,
      "avg_time": 10.968570613861084
    },
    "trabajo_fin": {
      "count": 7,
      "precision_at_k": 0.8571428571428571,
      "avg_relevancia": 0.8571428571428571,
      "avg_fidelidad": 0.7999999999999999,
      "avg_precision": 0.6571428571428571,
      "avg_overall": 0.7714285714285715,
      "avg_time": 7.155546358653477
    },
    "master": {
      "count": 4,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8,
      "avg_fidelidad": 0.825,
      "avg_precision": 0.4,
      "avg_overall": 0.675,
      "avg_time": 7.253016352653503
    },
    "defensoria": {
      "count": 2,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.85,
      "avg_fidelidad": 0.8500000000000001,
      "avg_precision": 0.75,
      "avg_overall": 0.8166666666666667,
      "avg_time": 9.80395781993866
    }
  },
  "metrics_by_question_type": {
    "factual": {
      "count": 95,
      "precision_at_k": 0.9052631578947369,
      "avg_relevancia": 0.8452631578947369,
      "avg_fidelidad": 0.7926315789473682,
      "avg_precision": 0.5947368421052631,
      "avg_overall": 0.7442105263157892,
      "avg_time": 6.270946683381733
    },
    "procedimental": {
      "count": 150,
      "precision_at_k": 0.7933333333333333,
      "avg_relevancia": 0.8453333333333345,
      "avg_fidelidad": 0.7746666666666671,
      "avg_precision": 0.6546666666666671,
      "avg_overall": 0.7582222222222215,
      "avg_time": 7.664699041048686
    }
  }
}