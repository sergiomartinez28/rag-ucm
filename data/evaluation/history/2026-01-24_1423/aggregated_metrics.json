{
  "precision_at_k": 0.72,
  "mrr": 0.6123333333333334,
  "avg_relevancia": 1.59,
  "avg_fidelidad": 1.58,
  "avg_precision": 1.55,
  "avg_overall": 1.5733333333333337,
  "avg_retrieval_time": 2.344068775653839,
  "avg_generation_time": 9.376275102615356,
  "avg_total_time": 11.720343878269196,
  "total_questions": 100,
  "questions_with_correct_doc": 72,
  "metrics_by_category": {
    "estatutos": {
      "count": 14,
      "precision_at_k": 0.5714285714285714,
      "avg_relevancia": 1.2857142857142858,
      "avg_fidelidad": 1.2857142857142858,
      "avg_precision": 1.2857142857142858,
      "avg_overall": 1.2857142857142858,
      "avg_time": 11.070678915296282
    },
    "general": {
      "count": 26,
      "precision_at_k": 0.7692307692307693,
      "avg_relevancia": 1.3461538461538463,
      "avg_fidelidad": 1.3461538461538463,
      "avg_precision": 1.2307692307692308,
      "avg_overall": 1.3076923076923077,
      "avg_time": 12.247782808083754
    },
    "reglamentos": {
      "count": 38,
      "precision_at_k": 0.631578947368421,
      "avg_relevancia": 1.7894736842105263,
      "avg_fidelidad": 1.736842105263158,
      "avg_precision": 1.7894736842105263,
      "avg_overall": 1.7719298245614037,
      "avg_time": 11.688525883775009
    },
    "trabajo_fin": {
      "count": 8,
      "precision_at_k": 0.875,
      "avg_relevancia": 1.5,
      "avg_fidelidad": 1.5,
      "avg_precision": 1.5,
      "avg_overall": 1.5,
      "avg_time": 11.307473808526993
    },
    "becas": {
      "count": 4,
      "precision_at_k": 0.75,
      "avg_relevancia": 1.0,
      "avg_fidelidad": 1.0,
      "avg_precision": 1.0,
      "avg_overall": 1.0,
      "avg_time": 9.811575949192047
    },
    "defensoria": {
      "count": 4,
      "precision_at_k": 1.0,
      "avg_relevancia": 1.5,
      "avg_fidelidad": 1.25,
      "avg_precision": 1.25,
      "avg_overall": 1.3333333333333335,
      "avg_time": 12.446179687976837
    },
    "gobierno": {
      "count": 6,
      "precision_at_k": 1.0,
      "avg_relevancia": 2.6666666666666665,
      "avg_fidelidad": 3.0,
      "avg_precision": 2.6666666666666665,
      "avg_overall": 2.7777777777777772,
      "avg_time": 12.491288900375366
    }
  },
  "metrics_by_question_type": {
    "procedimental": {
      "count": 30,
      "precision_at_k": 0.7,
      "avg_relevancia": 1.7666666666666666,
      "avg_fidelidad": 1.7,
      "avg_precision": 1.7,
      "avg_overall": 1.722222222222222,
      "avg_time": 12.147951141993206
    },
    "factual": {
      "count": 70,
      "precision_at_k": 0.7285714285714285,
      "avg_relevancia": 1.5142857142857142,
      "avg_fidelidad": 1.5285714285714285,
      "avg_precision": 1.4857142857142858,
      "avg_overall": 1.5095238095238095,
      "avg_time": 11.537083622387478
    }
  }
}