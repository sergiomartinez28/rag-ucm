{
  "precision_at_k": 0.7478632478632479,
  "mrr": 0.5552350427350428,
  "avg_relevancia": 0.7188034188034176,
  "avg_fidelidad": 0.7425213675213668,
  "avg_precision": 0.6389957264957256,
  "avg_overall": 0.700106837606839,
  "avg_retrieval_time": 8.248721399368383,
  "avg_generation_time": 11.559961683220333,
  "avg_total_time": 19.80940406699466,
  "total_questions": 468,
  "questions_with_correct_doc": 350,
  "metrics_by_category": {
    "gobierno": {
      "count": 58,
      "precision_at_k": 0.9137931034482759,
      "avg_relevancia": 0.7189655172413791,
      "avg_fidelidad": 0.7551724137931032,
      "avg_precision": 0.6370689655172417,
      "avg_overall": 0.7037356321839081,
      "avg_time": 19.62675061719171
    },
    "estatutos": {
      "count": 73,
      "precision_at_k": 0.726027397260274,
      "avg_relevancia": 0.7424657534246573,
      "avg_fidelidad": 0.7589041095890408,
      "avg_precision": 0.6671232876712335,
      "avg_overall": 0.7228310502283104,
      "avg_time": 19.67037847923906
    },
    "general": {
      "count": 134,
      "precision_at_k": 0.8432835820895522,
      "avg_relevancia": 0.7149253731343287,
      "avg_fidelidad": 0.7313432835820902,
      "avg_precision": 0.6283582089552238,
      "avg_overall": 0.6915422885572137,
      "avg_time": 19.906578279253264
    },
    "reglamentos": {
      "count": 171,
      "precision_at_k": 0.6140350877192983,
      "avg_relevancia": 0.7087719298245623,
      "avg_fidelidad": 0.7374269005847957,
      "avg_precision": 0.6403508771929823,
      "avg_overall": 0.6955165692007788,
      "avg_time": 19.867132778056185
    },
    "tfg": {
      "count": 11,
      "precision_at_k": 0.7272727272727273,
      "avg_relevancia": 0.7636363636363637,
      "avg_fidelidad": 0.7363636363636364,
      "avg_precision": 0.6272727272727272,
      "avg_overall": 0.7090909090909091,
      "avg_time": 19.77875531803478
    },
    "becas": {
      "count": 9,
      "precision_at_k": 0.6666666666666666,
      "avg_relevancia": 0.6777777777777778,
      "avg_fidelidad": 0.7333333333333333,
      "avg_precision": 0.5888888888888888,
      "avg_overall": 0.6666666666666665,
      "avg_time": 19.853217866685654
    },
    "defensoria": {
      "count": 6,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.6666666666666666,
      "avg_fidelidad": 0.7999999999999999,
      "avg_precision": 0.5666666666666667,
      "avg_overall": 0.6777777777777777,
      "avg_time": 18.418604532877605
    },
    "master": {
      "count": 6,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8333333333333335,
      "avg_fidelidad": 0.7833333333333333,
      "avg_precision": 0.6833333333333332,
      "avg_overall": 0.7666666666666667,
      "avg_time": 20.832307934761047
    }
  },
  "metrics_by_question_type": {
    "factual": {
      "count": 197,
      "precision_at_k": 0.6954314720812182,
      "avg_relevancia": 0.7172588832487315,
      "avg_fidelidad": 0.7472081218274113,
      "avg_precision": 0.6403553299492384,
      "avg_overall": 0.7016074450084594,
      "avg_time": 20.09048855486255
    },
    "procedimental": {
      "count": 269,
      "precision_at_k": 0.7843866171003717,
      "avg_relevancia": 0.7193308550185876,
      "avg_fidelidad": 0.7390334572490717,
      "avg_precision": 0.6375464684014853,
      "avg_overall": 0.6986369268897145,
      "avg_time": 19.59856865131279
    },
    "unknown": {
      "count": 2,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8,
      "avg_fidelidad": 0.75,
      "avg_precision": 0.7,
      "avg_overall": 0.7500000000000001,
      "avg_time": 20.479945421218872
    }
  }
}