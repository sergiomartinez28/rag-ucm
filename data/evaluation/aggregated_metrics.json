{
  "precision_at_k": 0.8326530612244898,
  "precision_at_k_chunk": 0.7551020408163265,
  "mrr": 0.704625850340136,
  "avg_relevancia": 0.8555102040816341,
  "avg_fidelidad": 0.8065306122448997,
  "avg_precision": 0.6567346938775513,
  "avg_overall": 0.7729251700680279,
  "avg_retrieval_time": 0.1516412219222711,
  "avg_generation_time": 6.683626377339266,
  "avg_total_time": 6.836121624343249,
  "total_questions": 245,
  "questions_with_correct_doc": 204,
  "questions_with_correct_chunk": 185,
  "abstention_rate": 0.0,
  "suspicious_short_rate": 0.0,
  "factual_accuracy": 0.673469387755102,
  "abstention_when_reference_exists": 0.0,
  "abstention_when_chunk_correct": 0.0,
  "abstention_when_doc_correct": 0.0,
  "abstention_with_numbers_in_ctx": 0.0,
  "abstention_with_keywords_in_ctx": 0.0,
  "metrics_by_category": {
    "estatutos": {
      "count": 39,
      "precision_at_k": 0.8205128205128205,
      "avg_relevancia": 0.8717948717948716,
      "avg_fidelidad": 0.8512820512820511,
      "avg_precision": 0.6641025641025639,
      "avg_overall": 0.7957264957264959,
      "avg_time": 6.283916907432752
    },
    "general": {
      "count": 74,
      "precision_at_k": 0.9459459459459459,
      "avg_relevancia": 0.8608108108108103,
      "avg_fidelidad": 0.8040540540540536,
      "avg_precision": 0.6972972972972971,
      "avg_overall": 0.787387387387387,
      "avg_time": 6.7560400995048315
    },
    "gobierno": {
      "count": 25,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.884,
      "avg_fidelidad": 0.8359999999999999,
      "avg_precision": 0.708,
      "avg_overall": 0.8093333333333333,
      "avg_time": 6.7703152275085445
    },
    "reglamentos": {
      "count": 89,
      "precision_at_k": 0.6853932584269663,
      "avg_relevancia": 0.8573033707865166,
      "avg_fidelidad": 0.7898876404494378,
      "avg_precision": 0.6415730337078651,
      "avg_overall": 0.7629213483146065,
      "avg_time": 6.683914873037446
    },
    "becas": {
      "count": 5,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8400000000000001,
      "avg_fidelidad": 0.86,
      "avg_precision": 0.5800000000000001,
      "avg_overall": 0.76,
      "avg_time": 10.363155269622803
    },
    "tfg": {
      "count": 7,
      "precision_at_k": 0.7142857142857143,
      "avg_relevancia": 0.7714285714285715,
      "avg_fidelidad": 0.7142857142857143,
      "avg_precision": 0.4999999999999999,
      "avg_overall": 0.6619047619047621,
      "avg_time": 10.105021272386823
    },
    "master": {
      "count": 4,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.575,
      "avg_fidelidad": 0.7,
      "avg_precision": 0.175,
      "avg_overall": 0.48333333333333345,
      "avg_time": 7.799393594264984
    },
    "defensoria": {
      "count": 2,
      "precision_at_k": 1.0,
      "avg_relevancia": 0.8,
      "avg_fidelidad": 0.8,
      "avg_precision": 0.75,
      "avg_overall": 0.7833333333333334,
      "avg_time": 5.977633595466614
    }
  },
  "metrics_by_question_type": {
    "factual": {
      "count": 95,
      "precision_at_k": 0.9157894736842105,
      "avg_relevancia": 0.8821052631578947,
      "avg_fidelidad": 0.8357894736842107,
      "avg_precision": 0.6799999999999999,
      "avg_overall": 0.7992982456140348,
      "avg_time": 6.010070077996505
    },
    "procedimental": {
      "count": 150,
      "precision_at_k": 0.78,
      "avg_relevancia": 0.8386666666666678,
      "avg_fidelidad": 0.7880000000000007,
      "avg_precision": 0.6420000000000002,
      "avg_overall": 0.756222222222222,
      "avg_time": 7.359287603696187
    }
  }
}